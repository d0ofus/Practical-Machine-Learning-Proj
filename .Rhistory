1+1
example <- matrix(c(1, 2, 3, 4, 5, 6, 7, 8), nrow = 4, ncol = 2)
View(example)
?iris
install.packages("ggplot2")
library(ggplot2)
library()
detach("package:ggplot2", unload = TRUE)
version
sessionInfo()
help(package="ggplot2")
browseVignettes("ggplot2")
library(devtools)
install.packages("devtools")
library(devtools)
help(package="devtools")
install.packages("KernSmooth")
library(KernSmooth)
?predict
search()
library(caret)
library(spam)
data(spam)
library(kernlab)
data(spam)
inTrain <- createDataPartition(y=spam$type, p=0.75, list=FALSE)
training <- spam[inTrain,]
testing <- spam[-inTrain,]
preProc <- preProcess(log10(training[,-58]+1), method="pca", pcaComp=2)
View(preProc)
view(preProc)
View(preProc)
class(preProc)
trainPC <- predict(preProc, log10(training[,-58]+1))
View(trainPC)
?predict
modelFit <- train(training$type ~., method="glm", data=trainPC)
?train
modelFit <- train(training$type ~ ., method="glm", data=trainPC)
modelFit
colnames(training)
names(training)
any(is.na(training))
modelFit <- train(training$type ~ ., method="glm", data=trainPC)
trainPC
trainPC
modelFit <- train(training$type ~ ., method="glm", data=trainPC)
testPC <- predict(preProc, log10(testing[,-58]+1))
View(testPC)
modelFit <- train(training$type ~., method="glm", data=trainPC)
View(trainPC)
modelFit <- train(training$type ~ PC1 + PC2, method="glm", data=trainPC)
any(is.na(trainPC))
names(trainPC)
training[type()]
training$type
warnings()
modelFit <- train(training$type ~ PC1 + PC2, method="glm", data=trainPC)
warnings()
modelFit <- train(training$type ~ ., method="glm", preProcess="pca", data=training)
modelFit <- train(training$type ~ .,method="glm",preProcess="pca",data=training)
modelFit <- train(type ~ ., method="glm", preProcess="pca", data=training)
confusionMatrix(testing$type, predict(modelFit, testing))
rm(list=ls())
library(ISLR); library(ggplot2);library(caret)
install.packages('ISLR')
library(ISLR); library(ggplot2);library(caret)
data(Wage)
force(Wage)
View(Wage)
Wage <- subset(Wage, select=-c(logwage))
?subset
names(Wage)
colnames(Wage)
summary(Wage)
data(Wage)
Wage <- subset(Wage, select=-c(logwage))
inTrain <- createDataPartition(y=Wage$wage, p=0.7, list=FALSE)
training <- Wage[inTrain,]
testing <- Wage[-inTrain,]
dim(training)
dim(testing)
featurePlot(x=training[,c("age","education","jobclass")],)
featurePlot(x=training[,c("age","education","jobclass")],
y = training$wage,
plot="pairs")
modFit <- train(wage ~ age + jobclass + education,
method="lm",data=training)
print(modFit)
summary(modFit)
View(modFit)
print(modFit$finalModel)
finMod <- modFit$finalModel
View(finMod)
View(modFit)
plot(finMod,1,pch=19,cex=0.5)
plot(finMod$residuals,pch=19)
library(AppliedPredictiveModeling)
install.packages('AppliedPredictiveModeling')
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
rm(list=ls())
data(AlzheimerDisease)
View(predictors)
adData = data.frame(diagnosis,predictors)
trainIndex=createDataPartition(diagnosis,p=0.5)
View(trainIndex)
trainIndex=createDataPartition(diagnosis,p=0.5, list=FALSE)
training=adData[trainIndex,]
testing <- adData[-trainIndex,]
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(1000)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
plot(mixtures$CompressiveStrength,pch=19)
library(Hmisc)
install.packages('Hmisc')
plot(mixtures$CompressiveStrength,pch=19, colour=colnames())
names(concrete)
plot(mixtures$CompressiveStrength,pch=19, colour=colnames(concrete))
hist(training$Superplasticizer)
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
names(adData)
trainingIL
trainingIL <- training[,grep("^IL|diagnosis", names(training))]
trainingIL <- training[,grep("^IL", names(training))]
preProc <- preProcess(trainingIL, method = 'pca', thresh = 0.8)
View(preProc)
?preProcess
preProc <- preProcess(trainingIL, method = 'pca', thresh = 0.9)
print(paste0('the number of principal components needed to capture 80% of the variance = ', preProc$numComp))
rm(list=ls())
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)data(AlzheimerDisease)
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]training = adData[ inTrain,]
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
set.seed(3433)
preProc <- preProcess(trainingIL, method = 'pca', thresh = 0.8)
trainingIL <- training[,grep("^IL|diagnosis", names(training))]
testingIL <- training[,grep("^IL|diagnosis", names(testing))]
set.seed(3433)
preProc <- preProcess(trainingIL, method = 'pca', thresh = 0.8)
fit <- train(diagnosis~., data=trainingIL, method="glm")
pred <- predict(fit, testingIL)
cm <- confusionMatrix(pred, testingIL$diagnosis)
cm
print(paste0("overall glm accuracy without PCA = ", cm$overall['Accuracy']))
fitPC <- train(diagnosis~., method="glm", data=trainingIL, preProcess="pca", trControl = trainControl(preProcOptions = list(thresh = 0.8)))
predPC <- predict(fitPC, testingIL)
cmPCA <- confusionMatrix(predPC, testingIL$diagnosis)
cmPCA
print(paste0("overall glm accuracy with PCA = ", cmPCA$overall['Accuracy']))
rm(list=ls9)
rm(list=ls())
data("iris")
library(ggplot2)
library("ggplot2")
names(iris)
View(iris)
inTrain = createDataPartition
library(caret)
inTrain = createDataPartition(y=iris$Species, p = 0.7, list=FALSE)
training <- iris[inTrain,]
testing <- iris[-inTrain,]
qplot(Petal.Width)
qplot(training$Petal.Width)
qplot(training$Petal.Width, training$Sepal.Width, colour=Species, data=training)
qplot(Petal.Width, Sepal.Width, colour=Species, data=training)
modFit <- train(Species ~., method="rpart", data=training)
print(modFit$finalModel)
View(modFit)
plot(modFit$finalModel, uniform=TRUE, main="Classification Tree")
text(modFit$finalModel, use.n=TRUE, all=TRUE, cex=.8)
library(rattle)
install.packages('rattle')
fancyRpartPlot(modFit$finalModel)
library(rattle)
fancyRpartPlot(modFit$finalModel)
predict(modFit, newdata=testing)
View(testing)
myModel <- predict(modFit, newdata=testing)
confusionMatrix(myModel, testing$Species)
print(paste0("Accuracy: ", round(accuracy, 2)))
# make this an external chunk that can be included in any file
options(width = 100)
opts_chunk$set(message = F, error = F, warning = F, comment = NA, fig.align = 'center', dpi = 100, tidy = F, cache.path = '.cache/', fig.path = 'fig/')
library(ElemStatLearn); data(ozone,package="ElemStatLearn")
install.packages("ElemStatLearn")
rm(list=ls())
library(ElemStatLearn); data(ozone,package="ElemStatLearn")
library(ElemStatLearn)
install.packages(ElemStatLearn)
install.packages("ElemStatLearn")
data(iris)
library(ggplot2)
inTrain <- createDataPartition(y=iris$Species, p=0.7, list=FALSE)
training<- iris[inTrain,]
testing<-iris[-inTrain,]
modFit<-train(Species ~., data=training, method="rf", prox=TRUE)
modFit<-train(Species ~., data=training, method="rf", prox=TRUE)
modFit
getTree(modFit$finalModel,k=2)
modFit$finalModel
library(partykit)
install.packages("partykit")
getTree(modFit$finalModel,k=2)
tree <- getTree(modFit$finalModel, k=2)
tree <- modFit$finalModel$trees[[1]]
tree
View(modFit)
tree <- modFit$finalModel$forest
tree <- modFit$finalModel$forest[1]
tree <- modFit$finalModel$forest[[1]
tree <- modFit$finalModel$forest[[1]]
tree <- modFit$finalModel$forest[[1]]
tree
pred <- predict(modFit, testing)
View(testing)
testing$predRight <- pred==testing$Species
table(pred,testing$Species)
qplot(Petal.Width, Petal.Length, colour=predRight, data=testing)
search()
library(ISLR)
data(Wage)
View(Wage)
Wage <- subset(Wage, select=-logwage)
inTrain<-createDataPartition(y=Wage$wage, p=0.7, list=FALSE)
training <- Wage[inTrain,]
testing<- Wage[-inTrain,]
rm(modFit)
modFit <- train(wage ~., method"gbm", data=training, verbose=FALSE)
modFit <- train(wage ~., method="gbm", data=training, verbose=FALSE)
modFit <- train(wage ~., method="gbm", data=training, verbose=FALSE)
modFit
qplot(predict(modFit, testing), wage, data=testing)
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
rm(list=ls())
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
library(rattle)
str(segmentationOriginal)
trainSet <- segmentationOriginal[segmentationOriginal$Case =="Train",]
testSet <- segmentationOriginal[segmentationOriginal$Case =="Test",]
set.seed(125)
model_rpart <- train(Class~.,data=trainSet,method="rpart")
fancyRpartPlot(model_rpart$finalModel)
library(pgmm)
install.packages("pgmm")
library(pgmm)
data(olive)
olive = olive[,-1]
newdata = as.data.frame(t(colMeans(olive)))
rm(testSet)
rm(trainSet)
View(olive)
str(olive)
table(olive$Area)
olive_rpart <- train(Area~.,data=olive,method="rpart")
fancyRpartPlot(olive_rpart$finalModel)
predict(olive_rpart,newdata=newdata)
prediction <- predict(oliver_rpart, newdata=newdata)
prediction <- predict(olive_rpart, newdata=newdata)
library(ElemStatLearn)
rm(list=ls())
library(ISLR)
data(Wage)
Wage <- subset(Wage, select=-logwage)
inBuild <- createDataPartition(y=Wage$wage, p=0.7, list=FALSE)
rm(list=ls())
source("~/.active-rstudio-document", echo=TRUE)
setwd("C:/Users/ervin/Desktop/Coursera/Data Science Specialization (JHU)/Practical_Machine_Learning/Project")
getwd()
source("~/.active-rstudio-document", echo=TRUE)
ead.csv("./data/pml-training.csv")
traincsv <- read.csv("./data/pml-training.csv")
testcsv <- read.csv("./data/pml-testing.csv")
traincsv <- read.csv("./data/pml-training.csv")
testcsv <- read.csv("./data/pml-testing.csv")
dim(traincsv)
str(traincsv)
traincsv <- traincsv[,colMeans(is.na(traincsv)) < .9] #Remove NA columns
head(traincsv)
colnames(traincsv)
traincsv[[1:5,c(1:7)]]
traincsv[1:5,c(1:7)]
traincsv <- traincsv[,-c(1:7)] #removing metadata which is irrelevant to the outcome
nvz <- nearZeroVar(traincsv)
nvz
traincsv[,nvz]
colnames(traincsv[nvz])
source("~/.active-rstudio-document", echo=TRUE)
nvz <- nearZeroVar(traincsv)
traincsv <- traincsv[,-nvz]
dim(traincsv)
summary(traincsv$classe)
unique(traincsv$classe)
inTrain <- createDataPartition(y=traincsv$classe, p=0.7, list=F)
train <- traincsv[inTrain,]
valid <- traincsv[-inTrain,]
?trainControl
# Set up control to train a 3-fold cv
control <- trainControl(method="cv", number=3, verboseIter=F)
control
# Run decision tree modeling
mod_trees <- train(classe~., data=train, method="rpart", trControl = control, tuneLength = 5)
fancyRpartPlot(mod_trees$finalModel)
pred_trees <- predict(mod_trees, valid)
cmtrees <- confusionMatrix(pred_trees, factor(valid$classe))
cmtrees
# Train a random forest model
mod_rf <- train(classe ~., method="rf", trControl = control, data=train, tuneLength = 5)
pred_rf <- predict(mod_rf, valid)
# Train a random forest model
mod_rf <- train(classe ~., method="rf", trControl = control, data=train, tuneLength = 5)
cmtrees
# Train a random forest model
mod_rf <- train(classe ~., method="rf", trControl = control, data=train, tuneLength = 5)
# Train a gradient boosted trees model
mod_gbm <- train(classe~., data=train, method="gbm", trControl = control, tuneLength = 5, verbose = F)
rm(list=ls()
rm(list=ls())
library(lattice)
search()
library(ggplot2)
search()
library(caret)
library(kernlab)
library(rattle)
library(corrplot)
set.seed(1111)
# Read datasets
traincsv <- read.csv("./data/pml-training.csv")
testcsv <- read.csv("./data/pml-testing.csv")
# Remove unneccesary variables
traincsv <- traincsv[,colMeans(is.na(traincsv)) < .9] #Remove NA columns
traincsv <- traincsv[,-c(1:7)] # Remove other unusable columns
nvz <- nearZeroVar(traincsv)
traincsv <- traincsv[,-nvz]
# Split into training/ validation subsets
inTrain <- createDataPartition(y=traincsv$classe, p=0.7, list=F)
train <- traincsv[inTrain,]
valid <- traincsv[-inTrain,]
# Set up control to train a 3-fold cv
control <- trainControl(method="cv", number=3, verboseIter=F)
# Train a decision tree model
mod_trees <- train(classe ~., method="rpart", trControl = control, data=train, tuneLength = 5)
pred_trees <- predict(mod_trees, valid)
cmtrees <- confusionMatrix(pred_trees, factor(valid$classe))
cmtrees
# Train a random forest model
mod_rf <- train(classe ~., method="rf", trControl = control, data=train, tuneLength = 5)
rm(list=ls())
set.seed(1111)
# Read datasets
traincsv <- read.csv("./data/pml-training.csv")
testcsv <- read.csv("./data/pml-testing.csv")
# dim(traincsv)
# Remove unneccesary variables
traincsv <- traincsv[,colMeans(is.na(traincsv)) < .9] #Remove NA columns
traincsv <- traincsv[,-c(1:7)] # Remove other unusable columns
nvz <- nearZeroVar(traincsv)
traincsv <- traincsv[,-nvz]
# dim(traincsv)
# Split into training/ validation subsets
inTrain <- createDataPartition(y=traincsv$classe, p=0.7, list=F)
train <- traincsv[inTrain,]
View(testcsv)
valid <- traincsv[-inTrain,]
# Set up control to train a 3-fold cv
control <- trainControl(method="cv", number=3, verboseIter=F)
# Train a random forest model
mod_rf <- train(classe ~., method="rf", trControl = control, data=train, tuneLength = 5)
rm(control)
# Train a decision tree model
mod_trees <- train(classe ~., method="rpart", data=train)
fancyRpartPlot(mod_trees$finalModel) # Plot final model
pred_trees <- predict(mod_trees, valid)
cmtrees <- confusionMatrix(pred_trees, factor(valid$classe))
cmtrees
# Train a random forest model
mod_rf <- train(classe ~., method="rf", data=train)
mod_rf
mod_rf <- randomForest(classe~., data=train)
# Train a random forest model
mod_rf <- train(classe ~., method='rf', data=train)
library(rmarkdown)
getwd()
render("final_report.Rmb")
render("final_report.Rmd")
render("final_report.Rmd")
